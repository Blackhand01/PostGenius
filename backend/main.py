import logging
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from models.requests import ContentRequest
from models.responses import ContentResponse
from services.news_retrieval import get_relevant_articles
from utils.content_generation import generate_social_post
from utils.image_generation import generate_image
from utils.video_generation import generate_video, generate_video_prompt_with_gpt
from utils.meme_generation import generate_meme
from services.vectara import index_vectara_document, search_documents

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

app = FastAPI(title="PostGenius API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/generate", response_model=ContentResponse)
async def generate_content(req: ContentRequest):
    logger.debug(f"Received request: prompt={req.prompt}, tone={req.tone}, platform={req.platform}")
    try:
        articles = get_relevant_articles(req.prompt, req.tone, req.platform)
        logger.debug(f"Articles retrieved: {articles}")

        if not articles:
            logger.warning("No articles found for the given prompt.")
            return ContentResponse(text="", image="", video="", meme="", sources=[])

        for document in articles:
            try:
                print("\n\n\n\n\n\n\n\n\n\n\n\n", document)
                # Index the document with embeddings on Vectara
                index_vectara_document(document)
            except Exception as e:
                print(f"Error processing document {document['id']}: {e}")

        summary = search_documents(req.prompt)
        logger.debug(f"Generated summary: {summary}")
        print("\n\n\n\n\n\n\n", summary)

        if not summary:
            logger.warning("No summary generated by LLM.")
            return ContentResponse(text="", image="", video="", meme="", sources=[])
        
        text_post = generate_social_post(summary, req.prompt, req.tone, req.platform)
        logger.debug(f"Generated text post: {text_post}")

        meme_url = generate_meme(summary, req.prompt, req.tone, req.platform)
        logger.debug(f"Generated meme url: {meme_url}")
        
        # Generate video prompt using GPT-4
        video_prompt = generate_video_prompt_with_gpt(summary, req.prompt, req.tone, req.platform)
        logger.debug(f"Generated video prompt: {video_prompt}")

        image_url = generate_image(summary, req.prompt, req.tone, req.platform)
        logger.debug(f"Generated image url: {image_url}")

        video_url = generate_video(video_prompt, image_url, duration=10)
        logger.debug(f"Generated video url: {video_url}")

        sources = [art["metadata"].get("source", "Source unavailable") for art in articles if "metadata" in art]
        logger.debug(f"Sources: {sources}")

        logger.debug(f"***END***")

        return ContentResponse(
            text=text_post or "",
            image=image_url or "",
            video=video_url or "",
            meme=meme_url or "",
            sources=sources
        )
        # return ContentResponse(
        #     text=text_post or "",
        #     image=image_url or "",
        #     video="",
        #     meme=meme_url or "",
        #     sources=[]
        # )

    except Exception as e:
        logger.exception(f"Error during content generation: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")
